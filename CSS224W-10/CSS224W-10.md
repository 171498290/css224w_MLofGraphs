# 图的深度生成模型(Deep Generative Models for Graphs CS224W 图机器学习)

## 🌎关于图生成的机器学习基础(Machine Learning for Graph Generation)
### 图生成模型(Graph Generativce Models)
+ 给定: 从$p_{data}(G)$中抽样的得到的图
+ 目标:
   + 学习$p_{model}(G)$的分布
   + 从$p_{model}(G)$中抽样

   <div align=center>
        <img src=1.png />
   </div>

---

###  生成模型重述(Recap: Generative Models)
+ Setup:
   + 假设我们想要从给定的数据点集合$\{x_i\}$中学习一个生成模型
      + $p_{data}(x)$是原始图的分布, 虽然这个分布是我们不知道的, 但我们有从原始图中抽样获得的点集$x_i \sim p_{data}(x)$

      + $p_{model}(x; \theta)$是训练得到的用来近似$p_{data}(x)$的分布

    + Goal:
       + (1) 使$p_{model}(x; \theta)$靠近$p_{data}(x)$
  
       + (2) 确保我们可以从$p_{model}(x; \theta)$中进行采样

+ (1) 如何使$p_{model}(x; \theta)$靠近$p_{data}(x)$:
   + 关键原则: 最大似然估计

   + 分布建模的基本方法
      + 找到参数$\theta^*$, 使得对于观察到的数据点 $x_i \sim p_{data}$, $\sum_i logp_{model}(x_i; \theta^*)$的值最大化. 也可以说是找到模型使得<font color=lightblue>其生成观测到的数据集的概率最大化</font>.

      $$\theta^* = arg\max_{\theta}\ \mathbb{E}_{x \sim p_{data}}log\ p_{model}(x|\theta)$$
   
+ (2) 如何从$p_{model}(x; \theta)$中采样:
   + 最普遍的方法:
      + 从一个简单的噪音分布中抽样
         $$z_i \sim N(0, 1)$$
      + 通过$f(\cdot)$变换噪音$z_i$
         $$x_i = f(z_i, \theta)$$

   + 如何定义$f(\cdot)$呢, 使用深度神经网络

+ <font color=lightblue>自回归模型(Autoregressive model, AR)</font>: 基于过去的行为来预测未来的行为
   + $p_{model}(x; \theta)$被用于<font color=lightblue>概率密度估计</font>(density estimation)和<font color=lightblue>基于概率密度采样</font>
      + 应用乘法公式: 联合概率是条件该路的乘积: 
         $$p_{model}(x; \theta) = \prod p_{model}(x_t|x_1,...,x_{t-1};\theta)$$

      + 这里的$x$可以是一个向量, $x_t$可以是其第$t$个分量. 比如$x$是一个句子, $x_t$是句子中的第$t$个词语.

      + 在当前的应用中, $x_t$表示的是第$t$个动作(添加节点, 添加边)

---

## 🌎GraphRNN
### GraphRNN Idea
+ 图的生成是通过有序的添加节点和边来完成的.

   <div align=center>
      <img src=2.png width=80%/>
   </div>

+ 节点序为$\pi$的图$G$可以唯一的映射成一个节点和边的添加序列$S^{\pi}$

   <div align=center>
      <img src=3.png width=80%/>
   </div>

---

### Model Graphs as Sequences
+ 序列$S^{\pi}$有两层含义:
   + 节点层(Node-level): 一次添加一个节点
   + 边层(Edge-level): 在已经存在的节点中添加边

+ Node-level: 在每一步, 有一个新的节点被添加

   <div align=center>
      <img src=4.png width=80%/>
   </div>

   + 每一个节点层的添加步骤都包含一个edge level的序列

   + Edge-level: 在每一步, 添加一个新的边

   <div align=center>
      <img src=5.png width=80%/>
   </div>

+ 上述过程将通过RNN来实现. 这样由序列生成图的过程, 就是在最大化生成观察到的图的概率, 同时还能使用训练好的模型进行新的输出.

---

### GrapRNN: Two levels of RNN
+ GraphRNN包含<font color=lightblue>node-level RNN</font>和<font color=lightblue>edge-level RNN</font>.

+ 两个RNN之间的联系:
   + Node-level RNN生成edge-level RNN的初始状态
   + Edge-level RNN为下一个新的节点生成边, 而后使用生成的结果更新node-level RNN的状态.

      <div align=center>
         <img src=6.png width=80%/>
      </div>

### RNN for Sequence Generation
+ 如何使用RNN来生成序列? 
   + 令$x_{t+1} = y_t$

+ 如何初始化$s_0, x_1$? 什么时候停止生成序列?
   + 使用开始(start sequence token)/结束序列标记(end sequence token), 例如零向量.

      <div align=center>
         <img src=7.png width=80%/>
      </div>

+ 使用RNN的目的依旧是最大化$\prod_{k = 1}^n p_{model}(x_t|x_1,...,x_{t-1};\theta)$

+ 令$y_t = p_{model}(x_t|x_1,...,x_{t-1};\theta)$

+ $x_{t+1}$是$y_t: x_{t+1}\sim y_t$的抽样
   + RNN每一步的输出都是一个概率向量
   + 从概率向量中进行抽样, 然后进行下一步的抽样

---

### RNN at Training Time
+ 我们观察一个变得序列$y^*$

+ 原理: <font color=lightblue>Teacher Forcing</font> -- 用实际序列代替输入和输出
      <div align=center>
         <img src=8.png width=80%/>
      </div>

+ 损失函数 $L$: 二元交叉熵(Binary cross entropy)损失函数

+ 最小化损失函数:
   $$L = -[y_1*log(y_1) + (1 - y_1^*)log(1 - y_1)]$$

---

### 训练实例
   <div align=center>
      <img src=9.png width=30%/>
   </div>

+ 假设节点1已经存在与图中了,现在的目的是添加节点2

+ Edge RNN预测的是节点2如何与节点1连接
+ Edge RNN被真实的连接序列监督

   <div align=center>
      <img src=10.png width=30%/>
      <img src=11.png width=30%/>
   </div>

+ 新的被用来更新Node RNN
   <div align=center>
      <img src=12.png width=50%/>
   </div>

+ Edge RNN预测的是节点3如何与前面节点连接
+ Edge RNN被真实的连接序列监督
   <div align=center>
      <img src=13.png width=45%/>
      <img src=14.png width=45%/>
   </div>

+ 继续进行上述步骤直到最后一个节点, 然后停止生成.
   <div align=center>
      <img src=15.png width=75%/>
   </div>

   <div align=center>
      <img src=16.png width=75%/>
   </div>