# 图表示学习(Graph Representation Learning CS224W 图机器学习)

## 🌎基于随机游走的节点向量嵌入(Random Walk Approaches to Node Embeddings)
+ 基于随机游走的节点向量嵌入和自然语言处理中的word2vec词向量嵌入类似, 依照词向量嵌入来理解这个方法比较好.
+ [论文原文](https://arxiv.org/pdf/1403.6652.pdf)

### 随机游走(Random Walk)
+ 随机游走获得的节点就像词向量训练中的句子一样, 每一个节点类比于句子中的一个词语.
+ 通过随机游走来获取大量的训练序列.

### 特征学习
+ 给定$G = (V, E)$

+ 训练目标是给出一个从节点到向量的映射$u\rightarrow \mathbb{R}^d$

+ 使用极大似然估计:
   + 其中$N_R(u)$是通过随机游走获得的某一个节点序列$u$的邻居节点.
   + $z$表示节点映射后的向量

$$\max_z \sum_{u\in V}logP(N_R(u)|Z_u)$$

+ 极大似然估计中的概率如果用softmax来表示的话在计算时会非常耗时, 所以选择使用负采样来进行表示.
+ 在词向量学习中中负采样使用高频词作为噪声词, 在基于随机漫步的节点向量学习中噪声节点是通过一个随机分布的函数来选取的.
$$P(Z_v|Z_u) \approx P(D=1|Z_v, Z_u)\cdot \prod_k P(D = 0|Z_i, Z_u)$$

+ 相应的softmax表示就可以近似转化为下式: 
$$log(\frac{exp(Z_u^TZ_v)}{\sum_{n\in V}exp(Z_u^TZ_n)}) \approx log(\sigma(Z_u^TZ_v)) - \sum_{i = 1}^klog(\sigma(Z_u^TZ_{n_i})), n_i \sim P_v$$
+ $k$的取值越大, 近似的效果哦就越好

### 算法步骤
<div align=center>
    <img src=1.png>
</div>

<div align=center>
    <img src=2.png>
</div>

+ 原文算法使用的是层序softmax做优化, 课程中使用的是负采样.

---

## 🌎node2vec
### Biased Walks
+ idea: 使用有弹性的(flexible)偏好随机漫步(biased random walks)来交替获得关于网络的局部和全局视图.

    <div align=center>
        <img src=3.png width=65%>
    </div>

+ 通过两种漫步策略可以获得在给定节点$u$时和漫步长度$k$时其邻居节点的集合.
   + $N_{BFS}(u) = {s_1, s_2, s_3}$, 局部的微观视图($|N_R(u)| = 3$)

   + $N_{DFS}(u) = {s_4, s_5, s_6}$, 全局的宏观试图($|N_R(u)| = 3$)

    <div align=center>
        <img src=4.png width=65%>
    </div>

---

### 模拟BFS和DFS
+ 两个参数:
   + <font color=lightblue>Return parameter</font> $p$:
      + 控制当前节点的下一次漫步时候会返回前一个结点

   + <font color=lightblue>In-out parameter</font> $q$:
      + 控制节点以类似BFS的模式游走还是以类似DFS的模式游走

+ 偏好2阶随机漫步(Biased $2^{nd}$-order wandom walks)如何扩展网络邻居:
   + 随机漫步当前到达的节点是$w$, 它是从节点$s_1$过来的.
   + 可以看到$w$下一步的漫步只能如下图所示:
      <div align=center>
          <img src=5.png width=65%>
      </div>
   + 接下来就有控制漫步走向的两个参数来决定下一步的走向.
      + 若节点$s_1$到与节点$w$相连的节点距离为0, 两个节点之间的<font color=lightgreen>未正则化概率</font>为$\frac{1}{p}$. 距离为0代表的就是当前节点$w$与其前一个结点$s_1$. $p$越小, 代表当前节点$w$返回前一个结点的概率越大.

      + 若节点$s_1$到与节点$w$相连的节点距离为1, 两个节点之间的<font color=lightgreen>未正则化概率</font>为$1$. 距离为1就代表与$w$相连的节点同时与其前一个结点$s_1$相连.

      + 若节点$s_1$到与节点$w$相连的节点距离为2, 两个节点之间的<font color=lightgreen>未正则化概率</font>为$\frac{1}{q}$. $p$越小, 这相当于对BFS的模拟.

      <div align=center>
       <img src=6.png width=65%>
      </div>

      + $BFS-like$ 漫步: $p$值较小
      + $DFS-like$ 漫步: $q$值较小

---

### 算法描述
+ 如图所示
<div align=center>
       <img src=7.png width=65%>
</div>