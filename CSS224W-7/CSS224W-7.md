# 图表示学习(Graph Representation Learning CS224W 图机器学习)

## 🌎基于随机游走的节点向量嵌入(Random Walk Approaches to Node Embeddings)
+ 基于随机游走的节点向量嵌入和自然语言处理中的word2vec词向量嵌入类似, 依照词向量嵌入来理解这个方法比较好.
+ [论文原文](https://arxiv.org/pdf/1403.6652.pdf)

### 随机游走(Random Walk)
+ 随机游走获得的节点就像词向量训练中的句子一样, 每一个节点类比于句子中的一个词语.
+ 通过随机游走来获取大量的训练序列.

### 特征学习
+ 给定$G = (V, E)$

+ 训练目标是给出一个从节点到向量的映射$u\rightarrow \mathbb{R}^d$

+ 使用极大似然估计:
   + 其中$N_R(u)$是通过随机游走获得的某一个节点序列$u$的邻居节点.
   + $z$表示节点映射后的向量

$$\max_z \sum_{u\in V}logP(N_R(u)|Z_u)$$

+ 极大似然估计中的概率如果用softmax来表示的话在计算时会非常耗时, 所以选择使用负采样来进行表示.
+ 在词向量学习中中负采样使用高频词作为噪声词, 在基于随机漫步的节点向量学习中噪声节点是通过一个随机分布的函数来选取的.
$$P(Z_v|Z_u) \approx P(D=1|Z_v, Z_u)\cdot \prod_k P(D = 0|Z_i, Z_u)$$

+ 相应的softmax表示就可以近似转化为下式: 
$$log(\frac{exp(Z_u^TZ_v)}{\sum_{n\in V}exp(Z_u^TZ_n)}) \approx log(\sigma(Z_u^TZ_v)) - \sum_{i = 1}^klog(\sigma(Z_u^TZ_{n_i})), n_i \sim P_v$$
+ $k$的取值越大, 近似的效果哦就越好

### 算法步骤
<div align=center>
    <img src=1.png>
</div>

<div align=center>
    <img src=2.png>
</div>

+ 原文算法使用的是层序softmax做优化, 课程中使用的是负采样.

---

